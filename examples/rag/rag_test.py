import os.path
import tempfile

import streamlit as st
from langchain.agents import create_react_agent, AgentExecutor
from langchain.memory import ConversationBufferMemory
from langchain.tools.retriever import create_retriever_tool
from langchain_community.callbacks.streamlit.streamlit_callback_handler import StreamlitCallbackHandler
from langchain_community.chat_message_histories import StreamlitChatMessageHistory
from langchain_community.document_loaders import TextLoader
from langchain_community.embeddings import DashScopeEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_core.prompts import PromptTemplate
from langchain_text_splitters import RecursiveCharacterTextSplitter

from examples.factory.llm import LLMType, LLMFactory


@st.cache_resource(ttl="1h")
def configure_retriever(files):
    docs = []
    tmp_dir = tempfile.TemporaryDirectory(dir="/tmp")
    for file in files:
        temp_fp = os.path.join(tmp_dir.name, file.name)
        with open(temp_fp, "wb") as f:
            f.write(file.getvalue())
        loader = TextLoader(temp_fp, encoding="utf-8")
        docs.extend(loader.load())

    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    splits = text_splitter.split_documents(docs)

    embeddings = DashScopeEmbeddings()
    vector_db = Chroma.from_documents(
        documents=splits,
        embedding=embeddings
    )

    return vector_db.as_retriever()

st.title("æ–‡æ¡£é—®ç­”")
st.set_page_config(
    page_title="æ–‡æ¡£é—®ç­”",
    page_icon="ğŸ¤–",
    layout="wide",
    initial_sidebar_state="auto",
    menu_items={
        'Get Help': 'https://www.extremelycoolapp.com/help',
        'Report a bug': "https://www.extremelycoolapp.com/bug",
        'About': "# This is a header. This is an *extremely* cool app!"
    }
)


upload_files = st.sidebar.file_uploader("ä¸Šä¼ æ–‡ä»¶", type=["pdf", "docx", "md", "txt"], accept_multiple_files=True)

if not upload_files:
    st.info("è¯·å…ˆä¸Šä¼ æ–‡ä»¶")
    st.stop()

retriever = configure_retriever(upload_files)

if "messages" not in st.session_state or not st.sidebar.button("æ¸…ç©ºèŠå¤©è®°å½•"):
    st.session_state["messages"] = [
        {"role": "assistant", "content": "ä½ å¥½ï¼Œæˆ‘æ˜¯ä¸€ä¸ªæ™ºèƒ½æ–‡æ¡£åŠ©ç†æœºå™¨äººï¼Œä½ å¯ä»¥å‘æˆ‘æé—®ä»»ä½•é—®é¢˜ã€‚"}
    ]

for msg in st.session_state.messages:
    st.chat_message(msg["role"]).write(msg["content"])

tool = create_retriever_tool(retriever, "æ–‡æ¡£æ£€ç´¢", "ç”¨äºæ£€ç´¢ç”¨æˆ·æå‡ºçš„é—®é¢˜ï¼Œå¹¶åŸºäºæ£€ç´¢åˆ°çš„æ–‡æ¡£å†…å®¹è¿›è¡Œå›å¤")
tools = [tool]

# æŒ‡ä»¤æ¨¡æ¿
instructions = """æ‚¨æ˜¯ä¸€ä¸ªè®¾è®¡ç”¨äºæŸ»è¯¢æ–‡æ¡£æ¥å›ç­”é—®é¢˜çš„ä»£ç†æ‚¨å¯ä»¥ä½¿ç”¨æ–‡æ¡£æ£€ç´¢å·¥å…·ï¼Œ
å¹¶åŸºäºæ£€ç´¢å†…å®¹æ¥å›ç­”é—®é¢˜æ‚¨å¯èƒ½ä¸æŸ¥è¯¢æ–‡æ¡£å°±çŸ¥é“ç­”æ¡ˆï¼Œä½†æ˜¯æ‚¨ä»ç„¶åº”è¯¥æŸ¥è¯¢æ–‡æ¡£æ¥è·å¾—ç­”æ¡ˆã€‚
å¦‚æœæ‚¨ä»æ–‡æ¡£ä¸­æ‰¾ä¸åˆ°ä»»ä½•ä¿¡æ¯ç”¨äºå›ç­”é—®é¢˜ï¼Œåˆ™åªéœ€è¿”å›â€œæŠ±æ­‰ï¼Œè¿™ä¸ªé—®é¢˜æˆ‘è¿˜ä¸çŸ¥é“ã€‚â€ä½œä¸ºç­”æ¡ˆã€‚
"""

# åŸºç¡€æç¤ºæ¨¡æ¿
base_prompt_template = """
{instructions}

TOOLS:
--------
You have access to the following tools:

{tools}

To use a tool, please use the following format:

ZWJ```
Thought: Do I need to use a tool? Yes
Action: the action to take, should be one of [{tool_names}]
Action Input: {input}
Observation: the result of the action
ZWJ```

When you have a response to say to the Human, or if you do not need to use a tool, you MUST use the format:

ZWJ```
Thought: Do I need to use a tool? 
NoFinal Answer: [your response here]
ZWJ```

Begin!

Previous conversation history:
{chat history}

New input: {input}
{agent_scratchpad}
"""


base_prompt = PromptTemplate.from_template(base_prompt_template)

prompt = base_prompt.partial(instructions=instructions)
llm_factory = LLMFactory(
    llm_type=LLMType.LLM_TYPE_QWENAI,
)

msgs = StreamlitChatMessageHistory()
memory = ConversationBufferMemory(
    memory_key="chat_history",
    return_messages=True,
    output_key="output",
    chat_memory=msgs
)
agent = create_react_agent(llm_factory.create_llm(), tools, prompt)
agent_executor = AgentExecutor(
    agent=agent,
    tools=tools,
    memory=memory,
    verbose=True,
    handle_parsing_errors="æ²¡æœ‰ä»çŸ¥è¯†åº“æ£€ç´¢åˆ°ç›¸ä¼¼çš„å†…å®¹"
)

user_query = st.chat_input(placeholder="è¯·è¾“å…¥é—®é¢˜")

if user_query:
    st.session_state.messages.append({"role": "user", "content": user_query})
    st.chat_message("user").write(user_query)
    with st.chat_message("assistant"):
        st.write("æ€è€ƒä¸­...")
        st_cb = StreamlitCallbackHandler(st.container())
        config = {"callbacks": [st_cb]}
        response = agent_executor.invoke({"input": user_query}, config=config)
        st.session_state.messages.append({"role": "assistant", "content": response["output"]})
        st.write(response["output"])

# app = FastAPI(title="æˆ‘çš„LangChainæœåŠ¡", version="1.0.0", description="LangChainæœåŠ¡")
#
# # add_routes(
# #     app,
# #     chain,
# #     path="/chain"
# # )
#
# if __name__ == '__main__':
#     uvicorn.run(app, host="0.0.0.0", port=8000)
