import os

import streamlit as st
from langchain.prompts import PromptTemplate
from langchain.text_splitter import CharacterTextSplitter
from langchain_community.document_loaders import TextLoader, PyPDFLoader
from langchain_community.embeddings import DashScopeEmbeddings
from langchain_core.document_loaders import BaseLoader
from langchain_core.vectorstores import InMemoryVectorStore, VectorStore

from examples.factory.ai_factory import create_ai

# Customize the layout
st.set_page_config(page_title="Local AI Chat Powered by Xinference", page_icon="ğŸ¤–", layout="wide")


# Write uploaded file in temp dir
def write_text_file(content, file_path: str):
    try:
        with open(file_path, 'wb') as file:
            file.write(content)
        return True
    except Exception as e:
        print(f"Error occurred while writing the file: {e}")
        return False


# Prepare prompt template
prompt_template = """
ä½¿ç”¨ä¸‹é¢çš„ä¸Šä¸‹æ–‡æ¥å›ç­”é—®é¢˜ã€‚
å¦‚æœä½ ä¸çŸ¥é“ç­”æ¡ˆï¼Œå°±è¯´ä½ ä¸çŸ¥é“ï¼Œä¸è¦ç¼–é€ ç­”æ¡ˆã€‚
{context}

é—®é¢˜: {question}

å›ç­”:
"""

prompt = PromptTemplate(template=prompt_template, input_variables=["context", "question"])

# Initialize the Xinference LLM & Embeddings
embeddings = DashScopeEmbeddings()

llm = create_ai()

st.title("ğŸ“„æ–‡æ¡£å¯¹è¯")
uploaded_file = st.file_uploader("ä¸Šä¼ æ–‡ä»¶", type=["txt", "pdf"])


def create_vector_store() -> VectorStore:
    return InMemoryVectorStore(embeddings)


if uploaded_file is not None:
    file_type = uploaded_file.type
    content = uploaded_file.getvalue()

    parent_path = "/tmp/agent"
    file_path = os.path.join(parent_path, uploaded_file.name)
    write_text_file(content, file_path)
    loader: BaseLoader = None
    print(f"æ–‡ä»¶ç±»å‹ï¼š{file_type}")
    if file_type == "application/pdf":
        loader = PyPDFLoader(file_path)
    elif file_type == "txt":
        loader = TextLoader(file_path)
    else:
        st.error("ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹")
        exit(1)

    docs = loader.load()
    text_splitter = CharacterTextSplitter(chunk_size=2000, chunk_overlap=50)
    texts = text_splitter.split_documents(docs)
    vector_store = create_vector_store()
    db = vector_store.from_documents(texts, embeddings)
    st.success("ä¸Šä¼ æ–‡æ¡£æˆåŠŸ")

    # Query through LLM
    question = st.text_input("æé—®", placeholder="è¯·é—®æˆ‘ä»»ä½•å…³äºæ–‡ç« çš„é—®é¢˜", disabled=not uploaded_file)
    if question:
        similar_doc = db.similarity_search(question, k=1)
        st.write("ç›¸å…³ä¸Šä¸‹æ–‡ï¼š")
        st.write(similar_doc)
        context = similar_doc[0].page_content
        query_llm = prompt | llm
        response = query_llm.invoke({"context": context, "question": question})
        st.write(f"å›ç­”ï¼š{response.content}")
